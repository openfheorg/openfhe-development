name: 'Default Builder'
description: 'A custom GitHub actions pipeline that wrappedup build, unit_test, benchmark and extract stages'
  
inputs:
  module_name:
    description: 'Name of the module'
    required: true
    default: 'default'
  cmake_args:
    description: 'Argument for cmake command'
    required: true
    default: ''
  native_backend:
    description: 'Size of NativeInteger [32|64|128]'
    required: false
    default: '64'
  mathbackend:
    description: 'Multiprecision backend for large-words [2|4|6]'
    required: false
  with_debug:
    description: 'Compile with debug'
    type: boolean
    default: false
    required: false
  with_tcm:
    description: 'Run tcm stages'
    type: boolean
    default: false
    required: false
  run_extras:
    description: 'Run allextras stages'
    type: boolean
    default: false
    required: false
  with_nativeopts:
    description: 'Enables compiler to use native optimizations'
    type: boolean
    default: false
    required: false
  run_benchmark:
    description: 'Run Benchmark stages'
    type: boolean
    default: false
    required: false

runs:
  # 
  using: "composite"
  steps:

    # -- cmake configuration
    - name: '${{ inputs.module_name }}_cmake'
      shell: bash
      run: |
        echo "${{ inputs.module_name }}_cmake"
        
        whoami
        
        # Create the correct cmake arg string
        cmake_args="-DNATIVE_SIZE=${{ inputs.native_backend }}"

        # Debugging use only
        echo "Input values for ${{inputs.module_name}}
        cmake_args=${{inputs.cmake_args}}
        native_backend=${{inputs.native_backend}}
        mathbackend=${{inputs.mathbackend}}
        with_debug=${{inputs.with_debug}}
        with_tcm=${{inputs.with_tcm}}
        run_extras=${{inputs.run_extras}}
        with_nativeopts=${{inputs.with_nativeopts}}
        run_benchmark=${{inputs.run_benchmark}}
        "

        # Append options based on inputs
        [[ "${{inputs.with_tcm}}" != "false" ]] && cmake_args="${cmake_args} -DWITH_TCM=ON"
        [[ "${{inputs.run_extras}}" != "false" ]] && cmake_args="${cmake_args} -DBUILD_EXTRAS=ON"
        [[ "${{inputs.with_nativeopts}}" != "false" ]] && cmake_args="${cmake_args} -DWITH_NATIVEOPT=ON"
        [[ "${{inputs.with_debug}}" != "false" ]] && cmake_args="${cmake_args} -DCMAKE_BUILD_TYPE=Debug"
        [[ "${{inputs.mathbackend}}" != "" ]] && cmake_args="${cmake_args} -DMATHBACKEND=${{ inputs.mathbackend }}"
        [[ "${{inputs.mathbackend}}" == "6" ]] && cmake_args="${cmake_args} -DWITH_NTL=ON"

        # Append manual pushed in cmake args from the input, this needs to be after the inputs
        # So that the input can override defaults
        make_args="${cmake_args} ${{ inputs.cmake_args }}"
        
        echo "CMAKE ARGS: ${cmake_args}"

        cmake -S . -B build ${cmake_args}

    # -- build
    - name: '${{ inputs.module_name }}_build'
      shell: bash
      run: |
        echo "${{ inputs.module_name }}_build"

        cd build
        
        # run only if with_tcm input is set
        if [[ "${{inputs.with_tcm}}" == "true" ]]; then
          make tcm -j $(nproc)
        fi
        
        # Build the code base
        make -j $(nproc) all

        # run only if run_extra input is set
        if [[ "${{inputs.run_extras}}" == "true" ]]; then
          make -j $(nproc) allextras
        fi

    # -- binfhe
    - name: '${{ inputs.module_name }}_test_binfhe'
      shell: bash
      run: |
        echo "${{ inputs.module_name }}_test_binfhe"
        pwd
        echo "LD_LIBRARY_PATH = ${LD_LIBRARY_PATH}"
        build/unittest/binfhe_tests --gtest_output=xml

    # -- test_core
    - name: '${{ inputs.module_name }}_test_core'
      shell: bash
      run: |
        echo "${{ inputs.module_name }}_test_core"
        pwd
        echo "LD_LIBRARY_PATH = ${LD_LIBRARY_PATH}"
        build/unittest/core_tests --gtest_output=xml

    # -- test_pke
    - name: '${{ inputs.module_name }}_test_pke'
      shell: bash
      run: |
        echo "${{ inputs.module_name }}_test_pke"
        pwd
        echo "LD_LIBRARY_PATH = ${LD_LIBRARY_PATH}"
        build/unittest/pke_tests --gtest_output=xml

    # -- benchmark
    - name: ${{ inputs.module_name }}_benchmark
      shell: bash
      if: ${{ inputs.run_benchmark == 'true' }}
      run: |
        echo "${{ inputs.module_name }}_benchmark"
        # Double guard
        if [[ "${{ inputs.run_benchmark }}" == "true" ]]; then
          export OMP_NUM_THREADS=1
          build/bin/benchmark/binfhe-ginx         --benchmark_out="binfhe-ginx_${{ github.sha }}.csv"         --benchmark_out_format=csv
          build/bin/benchmark/lib-benchmark       --benchmark_out="lib-benchmark_${{ github.sha }}.csv"       --benchmark_out_format=csv
          build/bin/benchmark/poly-benchmark-1k   --benchmark_out="poly-benchmark-1k_${{ github.sha }}.csv"   --benchmark_out_format=csv
          build/bin/benchmark/poly-benchmark-4k   --benchmark_out="poly-benchmark-4k_${{ github.sha }}.csv"   --benchmark_out_format=csv
          build/bin/benchmark/poly-benchmark-16k  --benchmark_out="poly-benchmark-16k_${{ github.sha }}.csv"  --benchmark_out_format=csv
          
          # Skipping these for time resource reasons.
          # build/bin/benchmark/binfhe-ap           --benchmark_out="binfhe-ap_${{ github.sha }}.csv"           --benchmark_out_format=csv
          # build/bin/benchmark/basic_test          --benchmark_out="basic_test_${{ github.sha }}.csv"          --benchmark_out_format=csv
          # build/bin/benchmark/Encoding            --benchmark_out="Encoding_${{ github.sha }}.csv"            --benchmark_out_format=csv
          # build/bin/benchmark/IntegerMath         --benchmark_out="IntegerMath_${{ github.sha }}.csv"         --benchmark_out_format=csv
          # build/bin/benchmark/Lattice             --benchmark_out="Lattice_${{ github.sha }}.csv"             --benchmark_out_format=csv
          # build/bin/benchmark/NbTheory            --benchmark_out="NbTheory_${{ github.sha }}.csv"            --benchmark_out_format=csv
          # build/bin/benchmark/VectorMath          --benchmark_out="VectorMath_${{ github.sha }}.csv"          --benchmark_out_format=csv
          unset OMP_NUM_THREADS
        fi

    # -- upload benchmark results, NOTE - this list needs to be maintained with the above output files
    - name: upload_benchmark_artifacts
      if: ${{ inputs.run_benchmark == 'true' }}
      uses: actions/upload-artifact@v2
      with:
        name: benchmark_vectormath-artifact
        path: |
          binfhe-ginx_${{ github.sha }}.csv
          lib-benchmark_${{ github.sha }}.csv
          poly-benchmark-1k_${{ github.sha }}.csv
          poly-benchmark-4k_${{ github.sha }}.csv
          poly-benchmark-16k_${{ github.sha }}.csv
        retention-days: 30

    # -- extract
    - name: '${{ inputs.module_name }}_extras_core'
      shell: bash
      if: ${{ inputs.run_extras == 'true' }}
      run: |
        echo "${{ inputs.module_name }}_extras_core"
        # Double guard
        if [[ "${{ inputs.run_extras }}" == "true" ]]; then
          ./build/bin/extras/core/dft
          ./build/bin/extras/core/math
          ./build/bin/extras/core/ntt1
          ./build/bin/extras/core/ntt2
        fi
    
    # -- cleanup
    - name: '${{ inputs.module_name }}_cleanup'
      shell: bash
      run: rm -rf build